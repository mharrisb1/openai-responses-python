# Responses

There are many ways to define the response for your mocked API request. To define a response you just need to set the `response` property to either a [partial](#partial), a [model](#model), an [HTTPX response](#httpx-response), or a [function](#function) that returns an HTTPX response. You can also ignore setting the response and a [default response](#default-response) will be used.

## Default Response

For all routes, but especially stateful routes, you can skip manually defining the response and a default response will be returned. The default response will have all required fields of the return object but will not include any meaningful values for fields that would have been generated by the LLM.

!!! tip

    For stateful routes that do not involve LLM generated fields it is actually recommended to *not* define the response. Doing so might actually result in an error.

## Partial

All routes have an associated *partial* object. Partials are just typed dictionaries representations of the response objects where fields are not required. Any field not defined by the user will be given a default value by merging the partial object with the default response object.

Let's look at an example:

```python linenums="1"
openai_mock.chat.completions.create.response = {
    "choices": [
        {
            "index": 0,
            "finish_reason": "stop",
            "message": {"content": "Hello! How can I help?", "role": "assistant"},
        }
    ]
}
```

In this example, we're explicitly defining what the completion choice field should look like in the response but we're not explicitly defining any of the other fields.

Thanks to Python's `TypedDict` type, autocompletion for field names are automatically supported in your text editor or IDE.

## Model

Along with partial objects, you can also define the response as a full Pydantic `BaseModel` object which is what the official Python library uses for defining resource types.

One use case for this is to manually set the `status` field on the run resource object for polling.

```python linenums="1"
# create run
run = client.beta.threads.runs.create(thread.id, assistant_id=assistant.id)

# manually change status and assign updated run as response for retrieve call
run.status = "in_progress"
openai_mock.beta.threads.runs.retrieve.response = run

# retrieve run
run = client.beta.threads.runs.retrieve(run.id, thread_id=thread.id)
assert run.status == "in_progress"
```

## HTTPX Response

You can also set the response to a raw HTTPX response object. This is more involved than using either a partial or model but can allow you to test things like server failures or other status codes.

For convenience, `openai_responses` provides a re-import of `httpx.Response`.

```python linenums="1"
import pytest

import openai
from openai import APIStatusError

import openai_responses
from openai_responses import OpenAIMock, Response


@openai_responses.mock()
def test_create_chat_completion_failure(openai_mock: OpenAIMock):
    openai_mock.chat.completions.create.response = Response(500)

    client = openai.Client(api_key="sk-fake123", max_retries=0)

    with pytest.raises(APIStatusError):
        client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "system", "content": "You are a helpful assistant."},
                {"role": "user", "content": "Hello!"},
            ],
        )
```

## Function

For more complex scenarios or for taking advantage of RESPX [side effects](https://lundberg.github.io/respx/guide/#mock-with-a-side-effect), you can also define the response as a function as long as that function returns an HTTPX response object.

The function's signature must match one of:

```
(request: httpx.Request) -> httpx.Response
(request: httpx.Request, route: respx.Route) -> httpx.Response
(request: httpx.Request, route: respx.Route, state: openai_responses.StateStore) -> httpx.Response
```

Again, for convenience, the necessary HTTPX and RESPX imports are re-imported and provided by this library.

Looking at a real-life example, this test simulates two failed calls before finally succeeding on the third call.

```python linenums="1"
import openai

import openai_responses
from openai_responses import OpenAIMock, Request, Response, Route
from openai_responses.helpers.builders.chat import chat_completion_from_create_request


def completion_with_failures(request: Request, route: Route) -> Response:
    """Simulate 2 failures before sending successful response"""
    if route.call_count < 2:
        return Response(500)

    completion = chat_completion_from_create_request(request, extra={"choices": []})

    return Response(201, json=completion.model_dump())


@openai_responses.mock()
def test_create_chat_completion(openai_mock: OpenAIMock):
    openai_mock.chat.completions.create.response = completion_with_failures

    client = openai.Client(api_key="sk-fake123", max_retries=3)
    client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": "Hello!"},
        ],
    )

    assert openai_mock.chat.completions.create.calls.call_count == 3
```

This example also makes use of [helpers](helpers.md) which are convenient utilities for common operations like creating a model from a request, storing data in the state store, etc.
